The Possible Parameters For A Simulation:
=========================================
parameters that I actually change have an "***" before them...
you can see the defaults of all the parameters in the function run_simulation_from_genome.

***n_steps - the number of steps in the simulation
gen_len - the length of the genome in the simulation (at least 2501)
linker_len - the linker_len parameter (the width of the sigmoid added to the ends of the nucleosome footprint)
nuc_width - the width of the nucleosome (before adding the sigmoids)
REB1_width - the width of the REB1 trans factor
ABF1_width - the width of the ABF1 trans factor
RAP1_width - the width of the RAP1 trans factor
slide_len - the amount of bps in a single slide
***poly_rate - the rate of the left sliding that is due to the polymerase
poly_pos - a vector indicating the positions of the polymerase (for example: 1200:2800)
REB1_a_rate - the REB1 assembly rate
REB1_e_rate - the REB1 eviction rate
ABF1_a_rate - the ABF1 assembly rate
ABF1_e_rate - the ABF1 eviction rate
RAP1_a_rate - the RAP1 assembly rate
RAP1_e_rate - the RAP1 eviction rate
nuc_base_a_rate - the global starting assembly rate of the nucs, before changes
                  from the binding sites
nuc_base_e_rate - the global starting eviction rate of the nucs, before changes
                  from the binding sites
nuc_base_r_rate - the global starting right rate of the nucs, before changes
                  from the binding sites
nuc_base_l_rate - the global starting left rate of the nucs, before changes
                  from the binding sites
***TF_evic_intensity - the factor that multiplies the convolution of the
                       TF sites on the nuc eviction rate
***RSC_evic_intensity - the factor that multiplies the convolution of the
                        PolyAT sites on the nuc eviction rate
***RSC_evic_length - the length of the convolution of the
                     PolyAT sites on the nuc eviction rate
***RSC_slide_intensity - the factor that multiplies the convolution of the
                         PolyAT sites on the nuc sliding rates
***RSC_slide_length - the length of the convolution of the
                      PolyAT sites on the nuc sliding rates

					  
The Simulation Stages:
======================
There are a few stages to a simulation (where its only input is a 2501-bp-long sequence of A/C/G/T):
1. Extracting the Poly(dA:dT) and binding sites from the sequence (using Extract_Sites_From_Gene).
2. Generate the left, right, eviction and assembly rates, given the different sites (using generate_rates_from_sites).
3. Running the simulation, given the sliding, eviction and sliding parameters (using Simulate).

Extracting The Sites:
=====================
For now, the extraction is not too sophisticated. For each TF, it searches for its motif in the sequence as a complete match, and also searches for partial matches (only the strong letters in the motif). Partial matches are half as effective as the full matches when we later do the convolutions.
Poly(dA:dT) patterns are any sequence of 5 As (or more) or 5 Ts (or more).
In the end we have a vector for each type of site, where there is 1s (or 0.5s) in the relevant positions in regard to the sequence.

In the future I would like to extract the sites a little bit more intelligently, but for now the TFs aren't used that much in my simulations, so it's not urgent.

Generating The Rates:
=====================
The TF sites are relevant for both the simulation itself (allowing TFs to bind to those positions) and also for the nucleosome rates. The Poly(dA:dT) sites only effect the nucleosome rates at the moment (RSC is not actually in the simulation).
What I do, is take the base rates and then add to them convolutions of the site vectors with different windows, that are defined by the simulation parameters:
1. TF sites effect the nucleosome eviction rate. The convolution is between the sites and a step vector which is the length of the TF plus the length of the nucleosome. The step vector is multiplied by the "TF_evic_intensity" parameter.
2. Poly(dA:dT) sites effect eviction and sliding. Here we also do a convolution with a step vector, but this time the vector can have different lengths (added to the base length) and different intensities, all dictated by the "RSC_evic_intensity", "RSC_evic_length", "RSC_slide_intensity" and "RSC_slide_length" parameters. The sliding parameters change so that PolyA pushes right and PolyT pushes left.
3. The Polymerase also effects the different rates in its location (downstream of the TSS). it adds a left sliding rate (the "Poly_Rate" parameter) and also weakens the effects of RSC and the TFs. It also increases the rate of TF eviction. All of these haven't been explored too much yet because I'm focusing on the NFR in my simulations so far.

In the future I would like to change the convolution windows to not be step vectors but maybe gaussians or sygmoids... And also start exploring using the TFs and Polymerase in the simulations.

The Simulation Itself:
======================
The simulation keeps a state that changes over time - it is a binary vector, where 1s signify nucleosome centers. Each place that has no nucleosome center in it (and there is room for another nucleosome to enter) has an assembly rate. Every nucleosome also has an eviction rate, left and right sliding rate. The full procedure of the simulation is documented in the "Simulate" function. In the end it returns the sum of all the states that were during the simulation (representing the time that each bp had a nuc center on it), along with other values which are less important.

Comparing The Simulation To The Data:
=====================================
Once we have the vector of centers from the simulation, we can compare it to the vector of reads from the experiment. Right now there is one major way to compare - using a likelihood feature.
The simulation is normalized and then multiplied by the amount of total reads. Then, we assume that that represents the average of nucs that we expect to see in each bp, and that the distribution is a poisson distribution. We check to see what is the chance that, given that distribution, we got the experiment data (looking only at the NFR region and in log scale), and that is the likelihood feature. All this is done in the "Compare_Sum_To_Data" function.
There were also attempts to make other features like +1 position, NFR width and so on, but they were put aside for now.

Running A Simulation:
=====================
If you want to play around manually with parameters, I made a script that you can run that plots out the results in the end. The script is "Test_Script", and you can change most parameters in it (along with how many simulations are run and combined into a single vector of nuc centers). It's pretty intuitive.


=====|||*|||=====


Running The Simulations On The Cluster:
=======================================
The cluster scripts currently are able to test different combinations of parameters and return the ones that best fit the data. All possible combinations of the given parameters are calculated - the script isn't very intelligent at the moment...
In order to run the simulation on the cluster, there are a few things that you need to do:
1. Set the parameters that you want to try
2. Push the changes in git and then pull from the CS computer
3. Run the simulation
4. Copy the result file to your machine and look at the results

Setting The Parameters:
=======================
The parameters are set in the "create_full_params" script. In it there are a few vectors that are defined, each representing all options for a parameter in the simulations. You need to set those to be what you want.
If you add new vectors/parameters, you'll need to also change the "cluster_sim" function, to access to correct indices in the "sim_params" variable where the function runs the simulation.
When you've finished setting the parameters - multiply all of the vector lengths to get the amount of simulations that will run. You need to enter that number into main_map.sh (the number of loop iterations) and in cluster_reduce.m (the number of iterations and the vector lengths).
If you changed the gene that we are comparing the simulation to (and getting the rates from), you'll need to put its id in the relevant place in map.sh and reduce.sh...

Running The Simulation:
=======================
The first thing you need to do after connecting to sulfur-gw, is run /cs/bd/Daniel/nuc_simulator/clustering/main_map.sh - this script runs all of your sbatch commands together. Each sbatch command is a simulation with one of the sets of parameters, which in turn runs the "cluster_sim" function. Each one of these saves an output matlab file to /cs/bd/Daniel/simulations/output.
After you see that all of the sbatch commands were done (using squeue on the cluster), you can check that there were no unusual errors in the log file at /cs/bd/Daniel/simulations/logs/log (there will be stuff written there, but make sure there are no matlab errors). If everything is ok, you can run /cs/bd/Daniel/nuc_simulator/clustering/main_reduce.sh - this script goes over all of the result files and finds the best simulation. It saves a final matlab file in the output folder which will be called results_<GeneID>.mat, which you should copy to your machine.

Examining The Results:
======================
Once you have the results file, you can import it to your matlab and then run the "Review_Simulation" script (change the gene_id to your gene first), which will give you a couple of plots of the simulation that had the best likelihood, so you can see what how good the fit was and so on.

If You Want To Clean All Of The Logs And Mat Files:
===================================================
just go to /cs/bd/Daniel/simulations and run a "make clean" command.


And that's it!